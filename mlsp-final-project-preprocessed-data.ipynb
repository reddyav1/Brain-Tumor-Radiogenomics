{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nLink to notebook:\nhttps://www.kaggle.com/parthvora/mlsp-final-project-preprocessed-data/\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Imports\nimport os\nfrom glob import glob\nimport random\n\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nimport cv2 as cv\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"### Read in data paths\n\nDATA_PATH = \"/kaggle/input/lggsegmentationpreprocessed/kaggle_3m - processed/\"\nimage_files = []\nmask_files = glob('../input/lggsegmentationpreprocessed/kaggle_3m - processed/*/*_mask*')\n\nfor i in mask_files:\n    image_files.append(i.replace('_mask',''))\n\n# Train test split\ndf = pd.DataFrame(data={\"images\": image_files, 'masks' : mask_files})\ndf_train, df_test = train_test_split(df,test_size = 0.1)\ndf_train, df_val = train_test_split(df_train,test_size = 0.2)\nprint(df_train.values.shape)\nprint(df_val.values.shape)\nprint(df_test.values.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" \n\nClass for MRI dataset.\n\nIn original code, images are cropped, padded, resized, and normalized.\n\nImages are also augmented by scaling, rotating, and horizontally flipping.\n\n\"\"\"\n\nclass MRITrainDataset(Dataset):\n    \n    # Takes in a dataframe of (image, mask) and a pytorch transform\n    def __init__(self, df, transform_params):\n        self.image_list = list(df.images)\n        self.mask_list = list(df.masks)\n        self.transform_params = transform_params\n    \n    def __len__(self):\n        return(len(self.image_list))\n    \n    def __getitem__(self, i):\n\n        # Read in image\n        image = cv.imread(self.image_list[i])\n        mask = cv.imread(self.mask_list[i], cv.IMREAD_GRAYSCALE)\n        \n        # Apply transform or convert image ndarray to tensor\n        if self.transform_params is not None:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n            image, mask = self.transform(image, mask)\n\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        \n        return image, mask\n    \n    def transform(self, img, mask):\n        \n        #(cropsize, padding, imsize, degrees, scale, pflip, mean, stdev) = self.transform_params  \n        (degrees, scale, pflip, mean, stdev) = self.transform_params\n        \n        # Convert to PIL image\n        img = transforms.ToPILImage()(img)\n        mask = transforms.ToPILImage()(mask)\n        \n        # Crop boundry by cropsize pixels\n        #img = transforms.CenterCrop(cropsize)(img)\n        #mask = transforms.CenterCrop(cropsize)(mask)\n        \n        # Pad boundary by padding pixels\n        #img = transforms.Pad(padding)(img)\n        #mask = transforms.Pad(padding)(mask)\n        \n        # Resize image?\n        #img = transforms.Resize(imsize)(img)\n        #mask = transforms.Resize(imsize)(mask)\n        \n        # Affine transform = rotation + scaling, ignoring translation\n        angle = np.random.uniform(low=degrees[0], high=degrees[1])\n        scale = np.random.uniform(low=scale[0], high=scale[1])    \n        img = TF.affine(img, angle=angle, scale=scale, translate=(0, 0), shear=0)\n        mask = TF.affine(mask, angle=angle, scale=scale, translate=(0, 0), shear=0)\n        \n        # Horizontal flip\n        if np.random.uniform(0, 1) < pflip:\n            img = TF.hflip(img)\n            mask = TF.hflip(mask)\n            \n        # Convert to tensor\n        img = transforms.ToTensor()(img)\n        mask = transforms.ToTensor()(mask)\n            \n        # Normalize img only\n        img = transforms.Normalize(mean, stdev)(img)\n        \n        return img, mask\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" \n\nClass for MRI dataset for validation and testing.\n\nJust normalizes images.\n\n\"\"\"\n\nclass MRITestDataset(Dataset):\n    \n    # Takes in a dataframe of (image, mask) and a pytorch transform\n    def __init__(self, df, transform_params):\n        self.image_list = list(df.images)\n        self.mask_list = list(df.masks)\n        self.transform_params = transform_params\n    \n    def __len__(self):\n        return(len(self.image_list))\n    \n    def __getitem__(self, i):\n\n        # Read in image\n        image = cv.imread(self.image_list[i])\n        mask = cv.imread(self.mask_list[i], cv.IMREAD_GRAYSCALE)\n        \n        # Apply transform or convert image ndarray to tensor\n        if self.transform_params is not None:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n            image, mask = self.transform(image, mask)\n\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        \n        return image, mask\n    \n    def transform(self, img, mask):\n        \n        (mean, stdev) = self.transform_params  \n            \n        # Normalize img only\n        img = transforms.Normalize(mean, stdev)(img)\n        \n        return img, mask\n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate mean and stdev of rgb values for training set\n\"\"\"\nmean_pixels = np.zeros(3)\nstd_pixels = np.zeros(3)\nfor fn in df_train.images:\n    \n    img = cv.imread(fn)\n    \n    mean_pixels += np.mean(img, axis=(0, 1))\n    std_pixels += np.std(img, axis=(0, 1))\n\n\nmean_pixels /= len(df_train.images)\nstd_pixels /= len(df_train.images)\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Initialize data transformer parameters\n#cropsize = 200\n#padding = 0\n#imsize = 256\ndegrees = (0, 0)\nscale = (1, 1)\npflip = 0.5\n\n# precalculated to speed up time\nmean = np.array([22.65424621, 19.91834147, 23.59800301])\nstdev = np.array([31.34933184, 30.29778424, 30.87938004])\n\n\ntransform_params = (\n\n    #cropsize,\n    #padding,\n    #imsize,\n    degrees, \n    scale,\n    pflip,\n    mean,\n    stdev\n\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Create MRI datasets and dataloaders\ntrain_dataset = MRITrainDataset(df_train, transform_params)\nval_dataset = MRITestDataset(df_val, (mean, stdev))\ntest_dataset = MRITestDataset(df_test, (mean, stdev))\n\n# Show some data\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\nfor step, (image, mask) in enumerate(train_loader):\n\n    \n    image = torch.reshape(image, (3, 224, 224))\n    mask = torch.reshape(mask, (224, 224))\n    \n    imx = transforms.ToPILImage()(image * 50 )\n    mkx = transforms.ToPILImage()(mask)\n    plt.figure()\n    plt.imshow(imx)\n    plt.figure()\n    plt.imshow(mkx)\n\n    if step == 4:\n        break\n\n# Reset to proper training batchsize\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taken from other code, computes dice loss\n\"\"\"\nclass DiceLoss(nn.Module):\n\n    def __init__(self):\n        super(DiceLoss, self).__init__()\n        self.smooth = 1.0\n\n    def forward(self, y_pred, y_true):\n        assert y_pred.size() == y_true.size()\n        y_pred = y_pred[:, 0].contiguous().view(-1)\n        y_true = y_true[:, 0].contiguous().view(-1)\n        intersection = (y_pred * y_true).sum()\n        dsc = (2. * intersection + self.smooth) / (\n            y_pred.sum() + y_true.sum() + self.smooth\n        )\n        return 1. - dsc\n\"\"\"\n\n# Also copied from somewhere else\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        #inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, train_loader, val_loader, epochs, lr, mean, stdev):\n    \n    # Can try different optimizers\n    optimizer = torch.optim.Adam(model.parameters(), lr)\n    compute_loss = DiceLoss()\n    \n    train_loss, val_loss = [], []\n    \n    if torch.cuda.is_available():\n        model = model.cuda()\n    else:\n        print(\"ERROR: CUDA NOT AVAILABLE\")\n        return [], []\n    \n    for epoch in range(epochs):\n        \n        # Train\n        tl = []\n        for step, (image, mask) in enumerate(train_loader):\n            \n            optimizer.zero_grad()\n            \n            image = image.cuda()\n            mask = mask.cuda()\n                \n            mask_hat = model(image)\n            loss = compute_loss(mask_hat, mask)\n            #print(loss.item())\n            tl.append(loss.item())\n            \n            loss.backward()\n            optimizer.step()\n            \n            del image, mask, mask_hat\n        train_loss.append(np.mean(tl))\n        \n        # Validate\n        vl = []\n        for step, (image, mask) in enumerate(val_loader):\n            \n            image = image.cuda()\n            mask = mask.cuda() \n                \n            mask_hat = model(image)\n            loss = compute_loss(mask_hat, mask)\n            vl.append(loss.item())\n            \n            del image, mask, mask_hat\n        val_loss.append(np.mean(vl))\n        \n        print(\"Epoch: {0}\".format(epoch + 1))\n        print(\"Train Loss: {0}\".format(train_loss[-1]))\n        print(\"Validation Loss: {0}\".format(val_loss[-1]))\n     \n    return train_loss, val_loss\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, test_loader, mean, stdev):\n    \n    compute_loss = DiceLoss()\n    test_loss = []\n    \n    if torch.cuda.is_available():\n        model = model.cuda()\n    else:\n        print(\"ERROR: CUDA NOT AVAILABLE\")\n        return []\n    \n    for step, (image, mask) in enumerate(test_loader):\n            \n        image = image.cuda()\n        mask = mask.cuda()\n        \n        mask_hat = model(image)\n        loss = compute_loss(mask_hat, mask)\n        test_loss.append(loss.item())\n        \n        del image, mask, mask_hat\n        \n    return test_loss  \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UNet(nn.Module):\n\n    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n        super(UNet, self).__init__()\n\n        features = init_features\n        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n\n        self.upconv4 = nn.ConvTranspose2d(\n            features * 16, features * 8, kernel_size=2, stride=2\n        )\n        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n        self.upconv3 = nn.ConvTranspose2d(\n            features * 8, features * 4, kernel_size=2, stride=2\n        )\n        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n        self.upconv2 = nn.ConvTranspose2d(\n            features * 4, features * 2, kernel_size=2, stride=2\n        )\n        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n        self.upconv1 = nn.ConvTranspose2d(\n            features * 2, features, kernel_size=2, stride=2\n        )\n        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n\n        self.conv = nn.Conv2d(\n            in_channels=features, out_channels=out_channels, kernel_size=1\n        )\n\n    def forward(self, x):\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool1(enc1))\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n        return torch.sigmoid(self.conv(dec1))\n\n    @staticmethod\n    def _block(in_channels, features, name):\n        return nn.Sequential(\n            OrderedDict(\n                [\n                    (\n                        name + \"conv1\",\n                        nn.Conv2d(\n                            in_channels=in_channels,\n                            out_channels=features,\n                            kernel_size=3,\n                            padding=1,\n                            bias=False,\n                        ),\n                    ),\n                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n                    (name + \"relu1\", nn.ReLU(inplace=True)),\n                    (\n                        name + \"conv2\",\n                        nn.Conv2d(\n                            in_channels=features,\n                            out_channels=features,\n                            kernel_size=3,\n                            padding=1,\n                            bias=False,\n                        ),\n                    ),\n                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n                    (name + \"relu2\", nn.ReLU(inplace=True)),\n                ]\n            )\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = UNet()\n\n# Params taken from other code\nepochs = 20\nlr = 0.0001\ntrain_loss, val_loss = train(model, train_loader, val_loader, epochs, lr, mean, stdev)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}