{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Imports\nimport os\nfrom glob import glob\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport cv2 as cv\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n\n#serene's imports\nfrom skimage import data, io, filters\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\n\n# from __future__ import division\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nfrom numpy import linalg\nfrom random import random\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vim: set ts=4 sts=4 sw=4 expandtab smartindent:\n#\n# This file was originally part of the octave-forge project\n# Ported to python by Luis Pedro Coelho <luis@luispedro.org> (February 2008)\n# Copyright (C) 2006       Soren Hauberg\n# Copyright (C) 2008-2010  Luis Pedro Coelho (Python port)\n# \n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2, or (at your option)\n# any later version.\n# \n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details. \n# \n# You should have received a copy of the GNU General Public License\n# along with this file.  If not, see <http://www.gnu.org/licenses/>.\n\n\n\n__all__ = ['bwperim']\n\ndef bwperim(bw, n=4):\n    \"\"\"\n    perim = bwperim(bw, n=4)\n    Find the perimeter of objects in binary images.\n    A pixel is part of an object perimeter if its value is one and there\n    is at least one zero-valued pixel in its neighborhood.\n    By default the neighborhood of a pixel is 4 nearest pixels, but\n    if `n` is set to 8 the 8 nearest pixels will be considered.\n    Parameters\n    ----------\n      bw : A black-and-white image\n      n : Connectivity. Must be 4 or 8 (default: 8)\n    Returns\n    -------\n      perim : A boolean image\n    \"\"\"\n\n    if n not in (4,8):\n        raise ValueError('mahotas.bwperim: n must be 4 or 8')\n    rows,cols = bw.shape\n\n    # Translate image by one pixel in all directions\n    north = np.zeros((rows,cols))\n    south = np.zeros((rows,cols))\n    west = np.zeros((rows,cols))\n    east = np.zeros((rows,cols))\n\n    north[:-1,:] = bw[1:,:]\n    south[1:,:]  = bw[:-1,:]\n    west[:,:-1]  = bw[:,1:]\n    east[:,1:]   = bw[:,:-1]\n    idx = (north == bw) & \\\n          (south == bw) & \\\n          (west  == bw) & \\\n          (east  == bw)\n    if n == 8:\n        north_east = np.zeros((rows, cols))\n        north_west = np.zeros((rows, cols))\n        south_east = np.zeros((rows, cols))\n        south_west = np.zeros((rows, cols))\n        north_east[:-1, 1:]   = bw[1:, :-1]\n        north_west[:-1, :-1] = bw[1:, 1:]\n        south_east[1:, 1:]     = bw[:-1, :-1]\n        south_west[1:, :-1]   = bw[:-1, 1:]\n        idx &= (north_east == bw) & \\\n               (south_east == bw) & \\\n               (south_west == bw) & \\\n               (north_west == bw)\n    return ~idx * bw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SERENE'S CODE\n\ndef find_centroid(mask1):\n    M = cv.moments(mask1) #all three parts of the mask are identical\n\n    # calculate x,y coordinate of center\n\n    X = int(M[\"m10\"] / M[\"m00\"])\n    Y = int(M[\"m01\"] / M[\"m00\"]) \n\n    return X,Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def std_one_bin(angle_vals, og_inds1, og_inds2, starting_angle, ending_angle, mean1, X, Y):\n    \n    \n    inds1 = og_inds1[ angle_vals < ending_angle ]\n    inds2 = og_inds2[ angle_vals < ending_angle ]\n    angle_vals2 = angle_vals[angle_vals < ending_angle ]\n    inds1 = inds1[ angle_vals2 >= starting_angle ]\n    inds2 = inds2[ angle_vals2 >= starting_angle ]\n    \n#     print(inds1)\n#     print(inds2)\n#     print(angle_vals2[angle_vals2 > starting_angle])\n    \n    radial_dists = np.sqrt(np.square((inds1 - Y)) + np.square(inds2 - X))/mean1\n    if (len((radial_dists))>0):\n        return np.std( radial_dists )\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ten_equiangular_bins(edges_all, x_center, y_center, mean_rad_dist):\n    final_bin_stds = np.empty((10,1))\n    \n    edges_top = edges_all[0:y_center-1 ,:]\n    [top_inds1, top_inds2] = np.where(edges_top >0.1)\n    \n    starting_vec = [1, 0] \n    starting_vec = starting_vec / np.linalg.norm(starting_vec)\n    \n    cos_vals = np.zeros(len(top_inds1))\n    for t in range(len(top_inds1)):\n        curr_vec = [top_inds2[t] - x_center, top_inds1[t] - y_center ] \n        curr_vec = curr_vec / np.linalg.norm(curr_vec)\n        cos_vals[t] = np.dot(starting_vec, curr_vec)\n    \n    pre1 = np.argsort(cos_vals)\n    top_inds1 = top_inds1[pre1]\n    top_inds2 = top_inds2[pre1]       \n#     print(cos_vals[pre1])\n    \n    \n    final_bin_stds[0] = std_one_bin( cos_vals, top_inds1, top_inds2, 0.809017,1 , mean_rad_dist, x_center, y_center )\n    final_bin_stds[1] = std_one_bin( cos_vals, top_inds1, top_inds2, 0.309017,0.809017 , mean_rad_dist, x_center, y_center )\n    final_bin_stds[2] = std_one_bin( cos_vals, top_inds1, top_inds2, -0.309017,0.309017 , mean_rad_dist, x_center, y_center )\n    final_bin_stds[3] = std_one_bin( cos_vals, top_inds1, top_inds2, -0.809017, -0.309017, mean_rad_dist, x_center, y_center )\n    final_bin_stds[4] = std_one_bin( cos_vals, top_inds1, top_inds2, -1, -0.809017, mean_rad_dist, x_center, y_center )\n\n\n\n    edges_bottom = edges_all[y_center:np.size(edges_all,0)-1 ,:]\n    [bottom_inds1, bottom_inds2] = np.where(edges_bottom >0.1)\n    bottom_inds1 = bottom_inds1 + y_center\n    cos_vals = np.zeros(len(bottom_inds1))\n    for t in range(len(bottom_inds1)):\n        curr_vec = [bottom_inds2[t] - x_center, bottom_inds1[t] - y_center ]  # don't need to offset y_center bc already done by cropping\n        curr_vec = curr_vec / np.linalg.norm(curr_vec)\n        cos_vals[t] = np.dot( curr_vec, starting_vec )\n        \n    \n    pre2 = np.argsort(cos_vals)\n    order2 = np.flip(pre2)\n    bottom_inds1 = bottom_inds1[order2]\n    bottom_inds2 = bottom_inds2[order2]\n#     print(cos_vals[order2])\n\n    final_bin_stds[5] = std_one_bin( cos_vals, bottom_inds1, bottom_inds2, 0.809017,1 , mean_rad_dist, x_center, y_center )\n    final_bin_stds[6] = std_one_bin( cos_vals, bottom_inds1, bottom_inds2, 0.309017,0.809017 , mean_rad_dist, x_center, y_center )\n    final_bin_stds[7] = std_one_bin( cos_vals, bottom_inds1, bottom_inds2, -0.309017,0.309017 , mean_rad_dist, x_center, y_center )\n    final_bin_stds[8] = std_one_bin( cos_vals, bottom_inds1, bottom_inds2, -0.809017, -0.309017, mean_rad_dist, x_center, y_center )\n    final_bin_stds[9] = std_one_bin( cos_vals, bottom_inds1, bottom_inds2, -1, -0.809017, mean_rad_dist, x_center, y_center )\n\n\n    return final_bin_stds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ASSUME THAT mask IS A 2D BINARY IMAGE OF WHERE THE TUMOR IS\n\n# OUTPUTS: False, 0 if no tumor in layer\n# True if tumor in layer\n\n# BASED ON ONE 2017 PAPER, ONLY NEED TO CALCULATE FOR THE \"images with the largest tumor cross section for each patient.\" \n\ndef angular_std(mask):\n    if np.sum(np.sum(mask))==0:\n        return False\n    else:\n        X,Y = find_centroid(mask)\n#         print(mask[Y,X]) # how to access centroid pt\n#         print(X,Y)\n        \n#         edges = filters.scharr(curr_im[:,:,0])\n        edges = bwperim(mask)\n        plt.figure()\n        plt.imshow(edges) \n        plt.scatter(X,Y, color='r')\n\n#         print(edges[119, 184])\n        [inds1, inds2] = np.where(edges >0.1)\n\n        radial_dists = np.sqrt(np.square((inds1 - Y)) + np.square(inds2 - X)) # DON'T WORRY, checked that inds1 ~ Y and inds2 ~ X\n#         print(radial_dists)\n        mean_dists = np.mean(radial_dists)        \n        radial_dists = radial_dists/mean_dists \n        \n        Rs_10 = ten_equiangular_bins(edges, X, Y, mean_dists)\n#         print(Rs_10)\n        final_ans = np.mean(Rs_10)\n        if np.isnan(final_ans):\n            final_ans =0\n        return final_ans\n\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sort_edges(edges_all, x_center, y_center):\n    edges_top = edges_all[0:y_center-1 ,:]\n    [top_inds1, top_inds2] = np.where(edges_top >0.1)\n    \n    starting_vec = [1, 0] \n    starting_vec = starting_vec / np.linalg.norm(starting_vec)\n    \n    cos_vals = np.zeros(len(top_inds1))\n    for t in range(len(top_inds1)):\n        curr_vec = [top_inds2[t] - x_center, top_inds1[t] - y_center ] \n        curr_vec = curr_vec / np.linalg.norm(curr_vec)\n        cos_vals[t] = np.dot(starting_vec, curr_vec)\n    \n    pre1 = np.argsort(cos_vals)\n    top_inds1 = top_inds1[pre1]\n    top_inds2 = top_inds2[pre1]       \n#     print(cos_vals[pre1])\n\n    edges_bottom = edges_all[y_center:np.size(edges_all,0)-1 ,:]\n    [bottom_inds1, bottom_inds2] = np.where(edges_bottom >0.1)\n    bottom_inds1 = bottom_inds1 + y_center\n    cos_vals = np.zeros(len(bottom_inds1))\n    for t in range(len(bottom_inds1)):\n        curr_vec = [bottom_inds2[t] - x_center, bottom_inds1[t] - y_center ]  # don't need to offset y_center bc already done by cropping\n        curr_vec = curr_vec / np.linalg.norm(curr_vec)\n        cos_vals[t] = np.dot( curr_vec, starting_vec )\n        \n    \n    pre2 = np.argsort(cos_vals)\n    order2 = np.flip(pre2)\n    bottom_inds1 = bottom_inds1[order2]\n    bottom_inds2 = bottom_inds2[order2]\n#     print(cos_vals[order2])\n\n    inds1 = np.concatenate((top_inds1, bottom_inds1))\n    inds2 = np.concatenate((top_inds2, bottom_inds2))\n    return inds1, inds2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def margin_fluctuation(mask):\n    if np.sum(np.sum(mask))==0:\n        return False\n    else:\n        X,Y = find_centroid(mask)\n        \n#         print(X,Y)\n\n#         [inds1, inds2] = np.where(edges >0.1)\n#         print(np.size(inds1))\n        \n        # want the order of indices to go radially rather than zigzagging over rows\n        \n        edges = bwperim(mask , 4)\n        y_inds, x_inds = sort_edges(edges, X, Y)\n        \n        radial_dists = np.sqrt(np.square((y_inds - Y)) + np.square(x_inds - X))\n        mean_dists = np.mean(radial_dists)        \n        radial_dists = radial_dists/mean_dists # normalize based on the avg radial distance to the centroid\n        \n        \n        p = int(0.1*len(y_inds)) # 10% of the length of the perimeter\n\n        padded_rs = np.concatenate(((radial_dists[len(radial_dists)-p:len(radial_dists)]),radial_dists))\n        padded_rs = np.concatenate((padded_rs, radial_dists[0:p]))\n        avged_radial_dists = np.convolve(padded_rs, np.ones(p)/p, mode='same') \n        avged_radial_dists = avged_radial_dists[p:len(padded_rs)-p] #Take away the padding that was originally used\n\n        \n        diff1 = avged_radial_dists - radial_dists #[p:len(padded_rs)-p]\n        \n        plt.figure()\n        plt.plot( range(len(radial_dists)), radial_dists,range(len(avged_radial_dists)), avged_radial_dists )\n        \n        stdev_all = np.std(diff1)\n        \n        \n        return stdev_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREDIT TO: https://github.com/minillinim/ellipsoid/blob/master/ellipsoid.py \n\nclass EllipsoidTool:\n    \"\"\"Some stuff for playing with ellipsoids\"\"\"\n    def __init__(self): pass\n    \n    def getMinVolEllipse(self, P=None, tolerance=0.01):\n        \"\"\" Find the minimum volume ellipsoid which holds all the points\n        \n        Based on work by Nima Moshtagh\n        http://www.mathworks.com/matlabcentral/fileexchange/9542\n        and also by looking at:\n        http://cctbx.sourceforge.net/current/python/scitbx.math.minimum_covering_ellipsoid.html\n        Which is based on the first reference anyway!\n        \n        Here, P is a numpy array of N dimensional points like this:\n        P = [[x,y,z,...], <-- one point per line\n             [x,y,z,...],\n             [x,y,z,...]]\n        \n        Returns:\n        (center, radii, rotation)\n        \n        \"\"\"\n        (N, d) = np.shape(P)\n        d = float(d)\n    \n        # Q will be our working array\n        Q = np.vstack([np.copy(P.T), np.ones(N)]) \n        QT = Q.T\n        \n        # initializations\n        err = 1.0 + tolerance\n        u = (1.0 / N) * np.ones(N)\n\n        # Khachiyan Algorithm\n        while err > tolerance:\n            V = np.dot(Q, np.dot(np.diag(u), QT))\n            M = np.diag(np.dot(QT , np.dot(linalg.inv(V), Q)))    # M the diagonal vector of an NxN matrix\n            j = np.argmax(M)\n            maximum = M[j]\n            step_size = (maximum - d - 1.0) / ((d + 1.0) * (maximum - 1.0))\n            new_u = (1.0 - step_size) * u\n            new_u[j] += step_size\n            err = np.linalg.norm(new_u - u)\n            u = new_u\n\n        # center of the ellipse \n        center = np.dot(P.T, u)\n    \n        # the A matrix for the ellipse\n        A = linalg.inv(\n                       np.dot(P.T, np.dot(np.diag(u), P)) - \n                       np.array([[a * b for b in center] for a in center])\n                       ) / d\n                       \n        # Get the values we'd like to return\n        U, s, rotation = linalg.svd(A)\n        radii = 1.0/np.sqrt(s)\n        \n        return (center, radii, rotation)\n\n    def getEllipsoidVolume(self, radii):\n        \"\"\"Calculate the volume of the blob\"\"\"\n        return 4./3.*np.pi*radii[0]*radii[1]*radii[2]\n\n    def plotEllipsoid(self, center, radii, rotation, ax=None, plotAxes=False, cageColor='b', cageAlpha=0.2):\n        \"\"\"Plot an ellipsoid\"\"\"\n        make_ax = ax == None\n        if make_ax:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, projection='3d')\n            \n        u = np.linspace(0.0, 2.0 * np.pi, 100)\n        v = np.linspace(0.0, np.pi, 100)\n        \n        # cartesian coordinates that correspond to the spherical angles:\n        x = radii[0] * np.outer(np.cos(u), np.sin(v))\n        y = radii[1] * np.outer(np.sin(u), np.sin(v))\n        z = radii[2] * np.outer(np.ones_like(u), np.cos(v))\n        # rotate accordingly\n        for i in range(len(x)):\n            for j in range(len(x)):\n                [x[i,j],y[i,j],z[i,j]] = np.dot([x[i,j],y[i,j],z[i,j]], rotation) + center\n    \n        if plotAxes:\n            # make some purdy axes\n            axes = np.array([[radii[0],0.0,0.0],\n                             [0.0,radii[1],0.0],\n                             [0.0,0.0,radii[2]]])\n            # rotate accordingly\n            for i in range(len(axes)):\n                axes[i] = np.dot(axes[i], rotation)\n    \n    \n            # plot axes\n            for p in axes:\n                X3 = np.linspace(-p[0], p[0], 100) + center[0]\n                Y3 = np.linspace(-p[1], p[1], 100) + center[1]\n                Z3 = np.linspace(-p[2], p[2], 100) + center[2]\n                ax.plot(X3, Y3, Z3, color=cageColor)\n    \n        # plot ellipsoid\n        ax.plot_wireframe(x, y, z,  rstride=4, cstride=4, color=cageColor, alpha=cageAlpha)\n        \n        if make_ax:\n            plt.show()\n            plt.close(fig)\n            del fig\n        \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bounding Ellipsoid Volume Ratio Calculation\n\ndef bevr(sorted_mask_list):\n    z_factor = 2; # the ratio of slice thickness to x-y pixel size\n    \n    # Build set of points \n    all_points = []\n    tumor_vol = 0;\n    for f in range(len(sorted_mask_list)): \n        curr_im = cv.imread(sorted_mask_list[f], cv.IMREAD_GRAYSCALE)\n        mask_pts = np.nonzero(curr_im)\n        \n        edges = bwperim(curr_im, 4)\n        edge_pts = np.nonzero(edges)\n        if np.size(edge_pts) > 0:\n            xs = edge_pts[0]\n            ys = edge_pts[1]\n            tumor_vol = tumor_vol + z_factor*len(mask_pts[0]) # Surface Area = number of points, Volume = z * surface area\n            for m in range(len(xs)):\n                pt_curr = [xs[m], ys[m], z_factor*f] \n                if np.size(all_points) > 0:\n                    all_points = np.append(all_points, pt_curr)\n                else:\n                    all_points = pt_curr\n\n    l = int(len(all_points)/3)\n    all_points = np.reshape(all_points,(l, 3))\n    ET = EllipsoidTool()\n    (center, radii, rotation) = ET.getMinVolEllipse(all_points, .01)\n    vol_ellipsoid =  ET.getEllipsoidVolume(radii)\n#     print(vol_ellipsoid)\n    \n    # Plotting code\n    \n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(all_points[:,0], all_points[:,1], all_points[:,2], color='g', marker='*', s=100)\n    ET.plotEllipsoid(center, radii, rotation, ax=ax, plotAxes=True)\n    plt.show()\n    plt.close(fig)\n    \n    final_ratio = tumor_vol/vol_ellipsoid\n    return final_ratio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/lgg-mri-segmentation/lgg-mri-segmentation/kaggle_3m/data.csv\")\nlabels = df.COCCluster.to_numpy()\nlabels -= 1\nprint(np.unique(labels))\n\npatients = []\ndata_dir = '/kaggle/input/lgg-mri-segmentation/kaggle_3m/'\nfor dirname, _, filenames in os.walk(data_dir):\n    if dirname == data_dir:\n        continue        \n    patients.append(dirname)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = np.empty((1, 3))\nidx = 0;\npatient = patients[2]\nfiles = os.listdir(patient)\n\nsorted_names = [None]*int(len(files)/2)\nlargest_tumor_section = np.zeros((1, 256, 256))\n\nfor i in files:\n    if \"mask\" in i:\n        fn = patient + \"/\" + i              \n        index = int(i.split(\"_\")[-2]) - 1\n        sorted_names[index] = fn \n\n        m = cv.imread(fn, cv.IMREAD_GRAYSCALE)    \n        curr_nonzs = np.size(np.nonzero(m))\n        old_nonzs = np.size(np.nonzero(largest_tumor_section))\n\n        if curr_nonzs > old_nonzs:\n            largest_tumor_section = m      \n\n # Only need to calculate 2D features from the largest tumor cross-section for each patient\nfeatures[idx,0] = angular_std(largest_tumor_section)\nfeatures[idx,1] = margin_fluctuation(largest_tumor_section)\nfeatures[idx,2] = (bevr(sorted_names))   \nprint(features[idx,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfeatures = np.empty((len(patients), 3))\nidx = 0;\nfor patient in patients:\n    files = os.listdir(patient)\n\n    sorted_names = [None]*int(len(files)/2)\n    largest_tumor_section = np.zeros((1, 256, 256))\n    \n    for i in files:\n        if \"mask\" in i:\n            fn = patient + \"/\" + i              \n            index = int(i.split(\"_\")[-2]) - 1\n            sorted_names[index] = fn \n            \n            m = cv.imread(fn, cv.IMREAD_GRAYSCALE)    \n            curr_nonzs = np.size(np.nonzero(m))\n            old_nonzs = np.size(np.nonzero(largest_tumor_section))\n            \n            if curr_nonzs > old_nonzs:\n                largest_tumor_section = m      \n            \n     # Only need to calculate 2D features from the largest tumor cross-section for each patient\n    features[idx,0] = angular_std(largest_tumor_section)\n#     features[idx,1] = margin_fluctuation(largest_tumor_section)\n#     features[idx,2] = (bevr(sorted_names))   \n#     print(features[idx,:])\n    \n    idx = idx+1;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('features_all_og.npy', features)\nnp.save('labels.npy', labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = np.load('../input/og-lgg-outputs/features_all_og.npy')\nlabs = np.load('../input/og-lgg-outputs/labels.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Custom linear regression \n# X: input features\n# y: data labels\n# spl: the chosen instance of ShuffleSplit or other splitter used\n\ndef train_lin_regr_custom(X,y, spl):    \n    y = y.ravel() # slightly modifies the shape of y    \n    curr_min_test_acc = 1e6  # will be set to the current lowest test accuracy    \n    best_params = [] # the ideal feature amount corresponding to lowest test accuracy will be saved here\n    \n    \n    tr_accs = []\n    test_accs = []\n    for train_index, test_index in spl.split(X,y):    \n        # Split up data into test and train data\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index] \n\n        # Normalization can help Linear Regression accuracy  \n        scl = MinMaxScaler() # subtract X_min then divide by (X_max - X_min)\n        X_train = scl.fit_transform(X_train) # set the mean + std for each feature and apply the scaler \n        X_test = scl.transform(X_test) #apply the scaler\n\n        # Initialize and fit model\n        model = LinearRegression(fit_intercept = True)\n        model = model.fit(X_train, y_train)\n\n        # Find model performance on training data\n        y_pred = model.predict(X_train)\n        tr1 = sum(y_train == np.round(y_pred))/len(y_train)\n\n        # Find model performance on testing data and append to lists\n        y_pred = model.predict(X_test)\n        te1 = sum(y_test == np.round(y_pred))/len(y_test)\n\n        tr_accs.append((tr1))\n        test_accs.append((te1)) \n\n\n    # Average test error across all n_splits\n    avg_te = sum(test_accs)/len(test_accs)\n\n    # Update \"best\" feature amount if testing RMSE is improved\n    if (curr_min_test_acc > avg_te):\n        curr_min_test_acc = avg_te\n\n#     # Print model results \n    print(\"AVG Testing:\", np.round(avg_te,3), \n                  \" AVG Training: \", np.round(sum(tr_accs)/len(tr_accs),3))\n\n    \n    print(\"\\nTrain size\", X_train.shape)\n    print(\"Test size\", X_test.shape)\n    # print(model.get_params())\n    return test_accs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Custom SVM Classifier \n# X: input features\n# y: data labels\n# spl: the chosen instance of ShuffleSplit or other splitter used\n\ndef train_SVM_custom(X,y, spl, c_custom):    \n    y = y.ravel() # slightly modifies the shape of y    \n    curr_min_test_acc = 1e6  # will be set to the current lowest test accuracy    \n    best_params = [] # the ideal feature amount corresponding to lowest test accuracy will be saved here\n    \n    \n    tr_accs = []\n    test_accs = []\n    for train_index, test_index in spl.split(X,y):    \n        # Split up data into test and train data\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index] \n\n        # Normalization can help   accuracy  \n        scl = StandardScaler() \n        X_train = scl.fit_transform(X_train) # set the mean + std for each feature and apply the scaler \n        X_test = scl.transform(X_test) #apply the scaler\n\n        # Initialize and fit model\n        model = svm.SVC(C=c_custom)\n        model = model.fit(X_train, y_train)\n\n        # Find model performance on training data\n        y_pred = model.predict(X_train)\n        tr1 = sum(y_train == np.round(y_pred))/len(y_train)\n\n        # Find model performance on testing data and append to lists\n        y_pred = model.predict(X_test)\n        te1 = sum(y_test == np.round(y_pred))/len(y_test)\n\n        tr_accs.append((tr1))\n        test_accs.append((te1)) \n\n\n    # Average test error across all n_splits\n    avg_te = sum(test_accs)/len(test_accs)\n\n    # Update \"best\" feature amount if testing RMSE is improved\n    if (curr_min_test_acc > avg_te):\n        curr_min_test_acc = avg_te\n\n#     # Print model results \n    print(\"AVG Testing:\", np.round(avg_te,3), \n                  \" AVG Training: \", np.round(sum(tr_accs)/len(tr_accs),3))\n\n    \n    print(\"\\nTrain size\", X_train.shape)\n    print(\"Test size\", X_test.shape)\n    # print(model.get_params())\n    return test_accs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_seed = 10;\nspl1 = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=rand_seed)\nall_kfold_accuracies = train_lin_regr_custom(feats,labs, spl1)\nprint(all_kfold_accuracies)\nall_kfold_accuracies = train_SVM_custom(feats,labs, spl1,1000)\nprint(all_kfold_accuracies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}